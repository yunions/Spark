# [1] Spark

## 1) Spark 자료구조

- RDD는 숙련도에 따라 성능이 달라지기 때문에, 데이터프레임 위주로 수업은 진행
- Main 자료 구조 : DataFrame - API - SQL

### 자료구조의 목적 : 저정&탐색

- Computer Science에서 자료구조는 프로그램 구동을 위한 기지(base)와 동일
- 프로그램 실행을 위한 실행코드, 기본 데이터 등은 모두 메모리에 적재
- 이때, 숫자/문자/문자열 등 각 타입에 따라 변수, 상수, 배열 및 구조체 등 형태에 따라 기본 저장 구조를 정함
- 산술, 논리 및 복잡한 함수 등을 활용한 다양한 연산을 통해 정렬, 조회 또는 탐색 및 요약 등의 알고리즘을 수행하는데 효과적인 연산구조를 결정

### Spark 자료구조, RDD는?

- 대용량 데이터의 병렬 및 분산 처리를 위한 저장 및 연산구조
- 여러 단계(stage)로 이루어진 복잡한 연산을 효과적으로 수행하기 위한 구조
- 각 단계의 자료공유에 디스크 대신 메모리를 사용하는 구조
- 한번 불러들이면 반복해서 여러 번 사용하는 **WORM(Write Once Read Many)**구조
- 연산 수행 중 메모리에 문제가 생겼을 때도 데이터를 살릴 수 있는 구조
- RAM을 Read-only로 사용하는 구조, 즉 RDD는 불변성(immutable)
- RDD는 불변성이지만 다른 RDD로의 변환(transformation)은 가능
- 즉 처음 로딩/생성한 RDD가 어떻게 변환돼 왔는지 계보(lineage)를 작성
- 이 계보를 통해 특정 시점으로 복원, 단계별 연산의 최적화 가능
- 변환은 기록만 하고, 실제 데이터를 요구할 때 연산 수행(action)

## 2) 구조적 API와 저수준 API

### 저수준(Low Level) API

- 일반적으로 이해하는 높고 낮음이 아닌 추상화의 레벨을 뜻함
- Low Level의 경우 더욱 구체적으로 세밀한 제어가 가능하다는 뜻

### 구조화된 컬렉션(Collection)

- DataFrame, Dataset는 구조화된 컬렉션으로 잘 정의된 row, column을 가짐
- 이 둘은 분산 테이블 형태를 가지며 지연연산의 실행계획, 즉 데이터 변환을 정의
- DataFrame은 스키마에 정의된 데이터 타입의 일치 여부를 런타임에 확인
- 반면, Dataset은 컴파일 타임에 스키마와 데이터 타입의 일치 여부를 확인
- DataFrame은 Row 타입으로 구성된 Dataset
  - Row 타입 : 스파크가 사용하는 연산에 최적화된 인메모리 포맷
- JVM 데이터 타입과 달리 가비지 컬렉션이나 객체 초기화 오버헤드 없음
- Scala를 사용해야 하는 이유 중 하나

## 3) 파티션과 불변성

### 파티션

- Spark는 병렬로 작업하기 위해 executor들에게 데이터를 분배
- 데이터는 파티션이라 불리는 청크(chunk) 단위로 분할
- 파티션은 물리적인 컴퓨터 각각에 위치하는 로우(Row)의 집합
- DataFrame API를 통해 몇 개의 파티션으로 처리해야 하는지 결정
- 실제 처리는 Spark가 결정

### 불변성과 변환

- DataFrame 등의 스파크 자료구조는 한 번 생성하면 변경할 수 없음
- 변경하는 대신 원본을 변환(transformation)하여 새로운 DataFrame을 생성
- 트랜스포메이션은 스파크에서 비즈니스 로직을 표현하는 핵심 개념
- 트랜스포메이션은 좁은 의존성과 넓은 의존성 두 가지가 있음

## 4) Transformation과 Action

### 논리적 실행계획

- 최초 원본 데이터를 적재한 뒤 단계별로 변환에 필요한 연산을 바로 수행하지 않음
- 단지 각 단계에서 요청한 변환에 대한 실행계획을 생성
- 액션 명령을 만나는 순간 논리적 실행계획을 간결한 물리적 실행계획으로 컴파일
- 이 때 전체 데이터의 흐름을 최적화

###
